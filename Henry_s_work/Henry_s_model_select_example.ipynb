{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Henry's model select demo\n",
    "\n",
    "The follow is the model select program base on Decision Tree Classfier.  \n",
    "Will be run on .py file instead of Jupyter Noteboook (for CSE server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-define basic parameter for system adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "TRAININGFILE = '../keyword.csv'\n",
    "TESTFILE = '../key_word_test.csv'\n",
    "TESTSIZE = 0.1\n",
    "\n",
    "x_label_list = ['article_words','key_word_10', 'key_word_20', 'key_word_50', 'key_word_100']\n",
    "y_label_list = ['topic']\n",
    "\n",
    "topic_code = {\n",
    "    'ARTS CULTURE ENTERTAINMENT': 1,\n",
    "    'BIOGRAPHIES PERSONALITIES PEOPLE': 2,\n",
    "    'DEFENCE': 3,\n",
    "    'DOMESTIC MARKETS': 4,\n",
    "    'FOREX MARKETS': 5,\n",
    "    'HEALTH': 6,\n",
    "    'MONEY MARKETS': 7,\n",
    "    'SCIENCE AND TECHNOLOGY': 8,\n",
    "    'SHARE LISTINGS': 9,\n",
    "    'SPORTS': 10,\n",
    "    'IRRELEVANT': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data_set from the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, x_label, y_label, split=False):\n",
    "    '''\n",
    "    Return the x and y columns for trainning\n",
    "    '''\n",
    "\n",
    "    if split == True:\n",
    "        train_set, test_set = train_test_split(df, test_size=TESTSIZE)\n",
    "        return train_set, test_set\n",
    "    else:\n",
    "        return df[[x_label, y_label]]\n",
    "\n",
    "\n",
    "# for the bag of word and label encode process\n",
    "def convert_word(bag_of_word_model, label_model, data_set, x_label, y_label='topic'):\n",
    "    '''\n",
    "    bow model need to be pre-fit when call current function\n",
    "    '''\n",
    "    act_x = bag_of_word_model.transform(data_set[x_label].values)\n",
    "    act_y = label_model.transform(data_set[y_label])\n",
    "\n",
    "    return act_x, act_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE with different *Bag of Word* model: \n",
    "1. CountVectorizer()\n",
    "2. TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_with_vector(df, vector_model, label_model, x_label):\n",
    "    '''\n",
    "    df                      data set\n",
    "    vector_model            Bag of Word model\n",
    "    x_label                 process x column\n",
    "    y_label                 process y column\n",
    "    '''\n",
    "    \n",
    "    count = vector_model.fit(df[x_label])\n",
    "\n",
    "    # convert the data\n",
    "    train_x, train_y = convert_word(count, label_model, df, x_label)\n",
    "\n",
    "    # start to SMOTE\n",
    "    smote = SMOTE(random_state=1)\n",
    "    sm_x, sm_y = smote.fit_sample(train_x, train_y)\n",
    "\n",
    "    # re-cover the data\n",
    "    new_x = count.inverse_transform(sm_x)\n",
    "    new_x = pd.Series([','.join(item) for item in new_x])\n",
    "\n",
    "    return new_x, sm_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the model pre-processing\n",
    "\n",
    "For **GridSearch** and also implement *StratifiedKFold* for cross-vaildation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(vector, model, train_x, train_y):\n",
    "    kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=1)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('vector', vector),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'model__splitter': ['best', 'random'],\n",
    "        'model__max_depth': range(10, 100),\n",
    "        'model__min_samples_split': range(2, 10),\n",
    "        'model__min_samples_leaf': range(1, 10),\n",
    "        'model__class_weight': [None, 'balanced']\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=kfold, n_jobs=-1)\n",
    "    grid_result=grid_search.fit(train_x, train_y)\n",
    "    return (grid_result.best_estimator_,grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Score function for model evaluate\n",
    "base on the topic on each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_score(model, label_model, data_set, topic_name, x_label):\n",
    "    test_data_set = data_set[data_set['topic'] == topic_name]\n",
    "    test_x = test_data_set[x_label]\n",
    "    test_y = test_data_set['topic']\n",
    "    pred_y = model.predict(test_x)\n",
    "\n",
    "    f1_score = metrics.f1_score(test_y, pred_y, average='macro')\n",
    "    accuarcy = metrics.accuracy_score(test_y, pred_y)\n",
    "    recall_score = metrics.recall_score(test_y, pred_y, average='macro')\n",
    "\n",
    "    return {\n",
    "        'f1': round(f1_score, 4),\n",
    "        'accuarcy': round(accuarcy, 4),\n",
    "        'recall_score': round(recall_score, 4)\n",
    "    }\n",
    "\n",
    "\n",
    "def model_score(model, label_model, x_label, test_df=None):\n",
    "    '''\n",
    "    model       The dt model\n",
    "    test_df     provide testing data set or using test file data\n",
    "    '''\n",
    "    \n",
    "    print('Topic\\tf1\\taccuarcy\\trecall_score')\n",
    "    test_report = []\n",
    "\n",
    "    # if using test file to train\n",
    "    if test_df == None:\n",
    "        test_df = pd.read_csv(TEST_FILE)\n",
    "        test_df = preprocess(test_df)\n",
    "\n",
    "    for topic in topic_code.keys():\n",
    "        result = [topic]\n",
    "        result.append(topic_score(model, label_model, test_df, topic, x_label))\n",
    "        test_report.append(result)\n",
    "\n",
    "    test_report.sort(reverse=True, key=lambda x: x[1]['accuarcy'])\n",
    "    for record in test_report:\n",
    "        print(record)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model save function\n",
    "The function will automatically save each trainning model and result report wait for further choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_job(model, test_report, pre_vector, feature_name):\n",
    "    filename = 'model/'+pre_vector+'_'+feature_name\n",
    "\n",
    "    joblib.dump(model, filename+'.model')\n",
    "    with open(filename+'.txt', 'w') as fp:\n",
    "        fp.write('Topic\\tf1\\taccuarcy\\trecall_score\\n')\n",
    "        for record in test_report:\n",
    "            fp.write(str(record)+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to implement the main function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compile(df, x_label, vector_num):\n",
    "    df = preprocess(df, x_label, 'topic')\n",
    "\n",
    "    label_model = preprocessing.LabelEncoder().fit(df['topic'])\n",
    "    encode_mapping = dict(zip(label_model.classes_, range(len(label_model.classes_))))\n",
    "\n",
    "    if vector_num == 1:\n",
    "        train_x, train_y = smote_with_vector(df, TfidfVectorizer(), label_model, x_label)\n",
    "    else:\n",
    "        train_x, train_y = smote_with_vector(df, CountVectorizer(), label_model, x_label)\n",
    "    topic = topic_code.keys()\n",
    "\n",
    "    # prepared for grid-search\n",
    "    count_dt_model, count_dt_accuarcy = grid_search(CountVectorizer(), DecisionTreeClassifier(), train_x, train_y)\n",
    "    tfidf_dt_model, tfidf_dt_accuarcy = grid_search(TfidfVectorizer(norm=None), DecisionTreeClassifier(), train_x, train_y)\n",
    "\n",
    "    if count_dt_accuarcy >= tfidf_dt_accuarcy:\n",
    "        print(f'*************************************************************')\n",
    "        print(f'Now the training set is {x_label}, and the model chosen is count_clf_NB')\n",
    "        print(f'The accuracy is {count_dt_accuarcy}')\n",
    "        return 'count',count_dt_model,label_model,encode_mapping\n",
    "    else:\n",
    "        print(f'*************************************************************')\n",
    "        print(f'Now the training set is {x_label}, and the model chosen is tfidf_clf_NB')\n",
    "        print(f'The accuracy is {tfidf_dt_accuarcy}')\n",
    "        return 'tfidf',tfidf_dt_model,label_model,encode_mapping\n",
    "\n",
    "def model_evaluate(model, x_label, label_model, df, encode_mapping, vector_num):\n",
    "    test_set = preprocess(df, x_label, 'topic')\n",
    "    test_x = test_set[x_label]\n",
    "    test_y = test_set[y_label]\n",
    "    topics = list(set(test_set['topic']))\n",
    "\n",
    "    # evalute total performance\n",
    "    pred_y = model.predict(test_x)\n",
    "    en_test_y = label_model.transform(test_y)\n",
    "    print('Total proformance')\n",
    "    print('F1 score:', metrics.f1_score(en_test_y, pred_y, average='macro'))\n",
    "    print('Accuarcy:', metrics.accuracy_score(en_test_y, pred_y))\n",
    "    print('Recall score:', metrics.recall_score(en_test_y, pred_y, average='macro'))\n",
    "    print('-'*15)\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(en_test_y, pred_y))\n",
    "\n",
    "    # evalute all the topic performance\n",
    "    model_report = model_score(model, x_label)\n",
    "\n",
    "    # save current model and performance\n",
    "    save_job(model, model_report, vector_num, x_label)\n",
    "\n",
    "    # for figure\n",
    "    conf_matrix = confusion_matrix(en_test_y, y_pred)\n",
    "    fig1 = plt.figure(figsize=(13,6))\n",
    "    sns.heatmap(conf_matrix,\n",
    "    #             square=True,\n",
    "                annot=True, # show numbers in each cell\n",
    "                fmt='d', # set number format to integer in each cell\n",
    "                yticklabels=le.classes_,\n",
    "                xticklabels=model.classes_,\n",
    "                cmap=\"Blues\",\n",
    "    #             linecolor=\"k\",\n",
    "                linewidths=.1,\n",
    "               )\n",
    "    plt.title(\n",
    "              f\"Confusion Matrix on Test Set | \" \n",
    "              f\"Classifier: {'+'.join([step for step in model.named_steps.keys()])}\", \n",
    "              fontsize=14)\n",
    "    plt.xlabel(\"Actual: False positives for y != x\", fontsize=12)\n",
    "    plt.ylabel(\"Prediction: False negatives for x != y\", fontsize=12)\n",
    "    plt.show()\n",
    "    plt.savefig('model/'+str(vector_num)+'_'+x_label+'.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start to test different model\n",
    "---\n",
    "\n",
    "For one topic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_label = 'key_word_100'\n",
    "\n",
    "df = pd.read_csv(TRAININGFILE)\n",
    "model, label_model, encode_mapping = model_compile(df, x_label)\n",
    "model_evaluate(model, x_label, label_model, df, encode_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mult-topic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(TRAININGFILE)\n",
    "for x_label in x_label_list:\n",
    "    for vector_num in [1, 2]:\n",
    "        model, label_model, encode_mapping = model_compile(df, x_label, vector_num)\n",
    "        model_evaluate(model, x_label, label_model, df, encode_mapping)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit440a9b05b92d4257b31359ad5d640545",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}